AWSTemplateFormatVersion: "2010-09-09"
Description: Creates the infrastructure for capturing Transit Gateway Flow Logs on exceptional traffic levels.

Parameters:
  GlobalNetworkArn:
    Type: String
    Description: If in NetworkManager mode, the global network ARN of the network to monitor, otherwise leave blank.
    AllowedPattern: "^$|^arn:[a-z-]+:networkmanager::[0-9]{12}:global-network/global-network-[a-z0-9]{17}$"
    ConstraintDescription: "Must be a global network ARN (arn:aws:networkmanager::accountid:global-network/global-network-0123456789abcdef)"
  TransitGatewayId:
    Type: String
    Description: If in TGW mode, the transit gateway ID to enabling alarming on attachments for, otherwise leave blank.
    AllowedPattern: "^$|^tgw-[0-9a-f]{17}$"
    ConstraintDescription: "Must be a valid Transit Gateway ID (e.g., tgw-0123456789abcdef0)"
  StdDev:
    Type: Number
    MinValue: 1
    MaxValue: 50
    Default: 2
    Description: By how many standard deviations should the anomaly detection threshold be set to?
  MinimumValue:
    Type: Number
    Default: 10000000
    Description: At very low usages, the standard deviation alarm will trigger on very low amounts. This parameter defines how many bytes per minute an attachment must have before we consider triggering on the detection threshold.
  MinutesBeforeStarting:
    Type: Number
    MinValue: 1
    Default: 2
    Description: How many minutes should the anomalies be occurring for before we alarm and start the capturing?
  ExistingSNSTopic:
    Type: String
    Description: If you have an existing SNS topic you want notifications sent to, provide the ARN here and ensure the lambda functions have permissions to publish to it, otherwise leave blank and provide EmailAddress instead.
    AllowedPattern: "^$|^arn:[a-z0-9-]+:sns:[a-z0-9-]+:[0-9]{12}:[a-zA-Z0-9_-]+$"
    ConstraintDescription: "Must be a valid SNS topic ARN (e.g., arn:aws:sns:us-east-1:123456789012:MyTopic) or empty"
  EmailAddress:
    Type: String
    Description: Optionally, enter an email address here you want notifications sent to when captures start and stop. A new SNS topic will be created.
    AllowedPattern: "^$|[^@]+@[^@]+\\.[^@]+"
    ConstraintDescription: "Must be a valid email address (e.g., user@example.com) or empty"
  MetricNamesToAlarm:
    Type: String
    Description: Comma-separated list of metrics to alarm on. The default just watches Bytes, but you may want to add in Packets.
    Default: BytesIn,BytesOut
  DaysToRetainFlowLogs:
    Type: Number
    Description: How many days should flow logs be left in the S3 buckets before being auto-deleted?
    MinValue: 1
    Default: 7
  DaysToRetainLambdaLogs:
    Type: Number
    Description: How many days should the Lambda functions retain their event logs in CloudWatch logs?
    MinValue: 1
    Default: 7
  ObjectsName:
    Type: String
    Description: A short unique string to add to all names (S3 buckets, Lambda functions, Alarm names) to uniquely identify this deployment. Must meet S3 bucket naming requirements (primarily all lowercase). No more than 16 letters to leave room for other IDs applied. 
    AllowedPattern: "(?!(^xn--|.+-s3alias$))^[a-z0-9][a-z0-9-]{1,16}[a-z0-9]$"
    Default: usagealarms

Rules:
  ValidateSNSConfiguration:
    Assertions:
      - Assert: !Not
          - !And
            - !Equals [!Ref ExistingSNSTopic, ""]
            - !Equals [!Ref EmailAddress, ""]
        AssertDescription: "You must provide either an ExistingSNSTopic ARN or an EmailAddress."
      - Assert: !Not
          - !And
            - !Not [!Equals [!Ref ExistingSNSTopic, ""]]
            - !Not [!Equals [!Ref EmailAddress, ""]]
        AssertDescription: "You must provide either an ExistingSNSTopic ARN or an EmailAddress, not both."
  ValidateObjectSelection:
    Assertions:
      - Assert: !Not
          - !And
            - !Equals [!Ref GlobalNetworkArn, ""]
            - !Equals [!Ref TransitGatewayId, ""]
        AssertDescription: "You must provide either a NetworkManager global network ARN, or a single Transit Gateway ID."
      - Assert: !Not
          - !And
            - !Not [!Equals [!Ref GlobalNetworkArn, ""]]
            - !Not [!Equals [!Ref TransitGatewayId, ""]]
        AssertDescription: "You must provide either a NetworkManager global network ARN, or a single Transit Gateway ID, not both."

Conditions:
  MakeSNSTopic:
    Fn::Not:
      - Fn::Equals:
        - !Ref EmailAddress
        - ""
  ModeNM:
    Fn::Equals:
      - !Ref TransitGatewayId
      - ""

Resources:
  S3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Join ['-', [ !Ref AWS::AccountId, !Ref ObjectsName, !Ref AWS::Region ] ]
      LifecycleConfiguration:
        Rules:
          - Id: !Sub "RemoveAfter${DaysToRetainFlowLogs}Days"
            Status: Enabled
            ExpirationInDays: !Ref DaysToRetainFlowLogs
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      ObjectLockEnabled: true
      ObjectLockConfiguration:
        ObjectLockEnabled: Enabled
        Rule:
          DefaultRetention:
            Mode: GOVERNANCE
            Days: !Ref DaysToRetainFlowLogs
      VersioningConfiguration:
        Status: Enabled

  S3Policy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref S3Bucket
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
         - Sid: "AWSLogDeliveryWrite"
           Effect: "Allow"
           Principal:
             Service: "delivery.logs.amazonaws.com"
           Action: "s3:PutObject"
           Resource: !Sub "${S3Bucket.Arn}/transit_gateway_logs/*"
           Condition:
             StringEquals:
               "s3:x-amz-acl": "bucket-owner-full-control"
               "aws:SourceAccount": !Sub "${AWS::AccountId}"
             ArnLike:
               "aws:SourceArn": !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
         - Sid: "AWSLogDeliveryCheck"
           Effect: "Allow"
           Principal:
             Service: "delivery.logs.amazonaws.com"
           Action:
             - "s3:GetBucketAcl"
             - "s3:ListBucket"
           Resource: !GetAtt S3Bucket.Arn
           Condition:
             StringEquals:
               "aws:SourceAccount": !Sub "${AWS::AccountId}"
             ArnLike:
               "aws:SourceArn": !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
         - Sid: "DenyInsecureConnections"
           Effect: "Deny"
           Principal: "*"
           Action: "s3:*"
           Resource:
             - !GetAtt S3Bucket.Arn
             - !Sub "${S3Bucket.Arn}/*"
           Condition:
             Bool:
               "aws:SecureTransport": "false"
         - Sid: "DenyPublicReadWrite"
           Effect: "Deny"
           Principal: "*"
           Action:
             - "s3:GetObject"
             - "s3:PutObject"
             - "s3:DeleteObject"
           Resource: !Sub "${S3Bucket.Arn}/*"
           Condition:
             StringNotEquals:
               "aws:SourceAccount": !Sub "${AWS::AccountId}"

  SNSEncryptionKey:
    Type: AWS::KMS::Key
    Condition: MakeSNSTopic
    Properties:
      Description: "KMS key for SNS topic encryption"
      KeyPolicy:
        Version: '2012-10-17'
        Statement:
          - Sid: Allow administration of the key
            Effect: Allow
            Principal:
              AWS: !Sub "arn:aws:iam::${AWS::AccountId}:root"
            Action: "kms:*"
            Resource: "*"
          - Sid: Allow SNS service
            Effect: Allow
            Principal:
              Service: sns.amazonaws.com
            Action:
              - "kms:Decrypt"
              - "kms:GenerateDataKey"
            Resource: "*"
          - Sid: Allow Lambda to use the key. We can't specify specific ARNs here due to dependency loops.
            Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action:
              - "kms:GenerateDataKey"
              - "kms:Decrypt"
            Resource: "*"
            Condition:
              StringEquals:
                "aws:SourceAccount": !Sub "${AWS::AccountId}"

  SNSTopic:
    Type: AWS::SNS::Topic
    Condition: MakeSNSTopic
    Properties:
      KmsMasterKeyId: !Ref SNSEncryptionKey
      Subscription:
        - Protocol: Email
          Endpoint: !Ref EmailAddress

  LambdaCommonPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      Description: !Sub Permissions needed for ${AWS::StackName} lambda functions to send to the SNS topic and perform logging
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: AllowSNSPublish
            Effect: Allow
            Action:
              - "sns:Publish"
            Resource: !If [MakeSNSTopic,!Ref SNSTopic,!Ref ExistingSNSTopic]
          - Sid: AllowLogging
            Effect: Allow
            Action:
              - logs:CreateLogStream
              - logs:PutLogEvents
            Resource: !GetAtt SharedLambdaLogGroup.Arn
          - !If
            - MakeSNSTopic
            - Sid: AllowKMSKeyUse
              Effect: Allow
              Action:
                - "kms:GenerateDataKey"
                - "kms:Decrypt"
              Resource: !GetAtt SNSEncryptionKey.Arn
            - !Ref "AWS::NoValue"

  SharedLambdaLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${ObjectsName}-shared-logs"
      RetentionInDays: !Ref DaysToRetainLambdaLogs

  EventHandlerModeNMPolicy:
    Type: AWS::IAM::ManagedPolicy
    Condition: ModeNM
    Properties:
      Description: !Sub Permissions needed for ${AWS::StackName}-EventHandlerLambda to function in NetworkManager mode and do cross-region deployment management
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: NetworkManagerOperations
            Effect: Allow
            Resource: "*"
            Action:
              - "networkmanager:GetTransitGatewayRegistrations"
              - "networkmanager:ListCoreNetworks"
              - "networkmanager:ListAttachments"
          - Sid: AllowCrossRegionS3BucketManagement
            Effect: Allow
            Resource: !Sub 'arn:${AWS::Partition}:s3:::${AWS::AccountId}-${ObjectsName}-*'
            Action:
              - "s3:CreateBucket"
              - "s3:DeleteBucket"
              - "s3:GetBucketPolicy"
              - "s3:PutBucketPolicy"
          - Sid: AllowCrossRegionLambdaManagement
            Effect: Allow
            Action:
              - "lambda:CreateFunction"
              - "lambda:DeleteFunction"
            Resource:
              - !Sub 'arn:${AWS::Partition}:lambda:*:${AWS::AccountId}:function:${ObjectsName}_Alarm_Handler'
          - Sid: AllowPassingRoleToLambdasInOtherRegions
            Effect: Allow
            Action:
              - "iam:PassRole"
            Resource: !GetAtt AlarmRole.Arn

  EventHandlerIAMPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      Description: !Sub Permissions needed for ${AWS::StackName}-EventHandlerLambda to function
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: AllowBucketAccess
            Effect: Allow
            Action:
              - "s3:DeleteObject"
              - "s3:ListBucket"
            Resource:
              - !Sub 'arn:${AWS::Partition}:s3:::${AWS::AccountId}-${ObjectsName}-*'
              - !Sub 'arn:${AWS::Partition}:s3:::${AWS::AccountId}-${ObjectsName}-*/*'
          - Sid: AllowTGWDescribe
            Effect: Allow
            Resource: "*"
            Action:
              - "ec2:DescribeTransitGatewayAttachments"
          - Sid: AllowCloudwatchAlarmDiscovery
            Effect: Allow
            Resource: !Sub 'arn:${AWS::Partition}:cloudwatch:*:${AWS::AccountId}:alarm:*'
            Action:
              - "cloudwatch:DescribeAlarms"
          - Sid: AllowFindingFunction
            Effect: Allow
            Action:
              - "lambda:GetFunction"
            Resource:
              - !Sub 'arn:${AWS::Partition}:lambda:*:${AWS::AccountId}:function:${ObjectsName}_Alarm_Handler'
          - Sid: AllowCloudwatchAlarmManipulation
            Effect: Allow
            Resource: !Sub 'arn:${AWS::Partition}:cloudwatch:*:${AWS::AccountId}:alarm:*-${ObjectsName}-Alarm'
            Action:
              - "cloudwatch:DescribeAlarms"
              - "cloudwatch:DeleteAlarms"
              - "cloudwatch:PutMetricAlarm"
              - "cloudwatch:SetAlarmState"
              - "cloudwatch:TagResource"

  EventHandlerRole:
    Type: AWS::IAM::Role
    Properties:
      Description: !Sub Role for ${AWS::StackName}, region ${AWS::Region}, lambda EventHandlerLambda
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal: {Service: [lambda.amazonaws.com]}
          Action: ['sts:AssumeRole']
      Path: "/"
      ManagedPolicyArns:
        - !Sub "arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
        - !Ref LambdaCommonPolicy
        - !Ref EventHandlerIAMPolicy
        - !If [ModeNM, !Ref EventHandlerModeNMPolicy, !Ref "AWS::NoValue"]

  EventHandlerLambda:
    Type: AWS::Lambda::Function
    DependsOn:
      - S3Bucket
      - S3Policy
      - AlarmLambda
    Properties:
      DeadLetterConfig:
        TargetArn: !If [MakeSNSTopic, !Ref SNSTopic, !Ref ExistingSNSTopic]
      LoggingConfig:
        LogGroup: !Ref SharedLambdaLogGroup
      Handler: index.handler
      Role: !GetAtt EventHandlerRole.Arn
      Runtime: python3.12
      # Give this script 10 minutes as especially the first run, with spreading out to all regions, can take a while.
      Timeout: 600
      FunctionName: !Sub "${ObjectsName}_Event_Handler"
      Environment:
        Variables:
          STACK_ARN: !Ref AWS::StackId
          METRIC_NAMES: !Ref MetricNamesToAlarm
          STDDEV: !Ref StdDev
          MINUTES: !Ref MinutesBeforeStarting
          MINIMUM_VALUE: !Ref MinimumValue
          SNS_TOPIC: !If [MakeSNSTopic,!Ref SNSTopic,!Ref ExistingSNSTopic]
          OBJECTS_NAME: !Ref ObjectsName
          ALARM_LAMBDA_ROLE: !GetAtt AlarmRole.Arn
          MODE: !If [ModeNM, "nm", "tgw"]
          GLOBAL_NETWORK_ARN: !Ref GlobalNetworkArn
          TGW_ID: !Ref TransitGatewayId
      Code:
        ZipFile: |
          """
          Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
          SPDX-License-Identifier: MIT-0
          
          event_handler.py
          
          Python lambda script to handle CloudFormation and EventBridge events for changes. This script relies on environment variables to pass in how
          things should be configured. These are normally set by the CloudFormation template wrapping this script. Those variables are:
          
          STACK_ARN - The CloudFormation stack ARN used for this deployment
          METRIC_NAMES - Comma-separated list of metrics to alarm on. ex: "BytesIn,BytesOut"
          STDDEV - By how many standard deviations should the anomaly detection threshold be set to, ex: "2"
          MINUTES - How many minutes should the anomalies be occurring for before we alarm and start the capturing?, ex: "2"
          MINIMUM_VALUE - At very low usages, the standard deviation alarm will trigger on very low amounts. This parameter defines how many bytes per minute an 
                          attachment must have before we consider triggering on the detection threshold.
          SNS_TOPIC - The SNS topic ARN to send notifications to
          OBJECTS_NAME - A short unique string to add to all names (S3 buckets, Lambda functions, Alarm names) to uniquely identify this deployment.
          ALARM_LAMBDA_ROLE - The IAM Role ARN to be used for the alarm-handling lambda functions
          TYPE - Set to 'nm' if we are using network manager, 'tgw' if a single TGW
          
          If TYPE='nm':
          GLOBAL_NETWORK_ARN - The global network ARN (arn:aws:networkmanager::accountid:global-network/global-network-0123456789abcdef) of the network to monitor.
          
          If TYPE='tgw':
          TGW_ID - The Transit Gateway ID to monitor
          
          Additionally, it leverages some variables set by AWS Lambda itself - AWS_LAMBDA_FUNCTION_NAME and AWS_REGION.
          """
          import boto3
          import boto3.exceptions
          import botocore
          import botocore.exceptions
          import cfnresponse
          import logging
          import threading
          import os
          import re
          import urllib.request
          from typing import Dict, List, Set, Any, Optional
          
          # 'tgw' or 'cwan' -> regions -> TGW or core network ID -> attachment ID -> dict of data (type:attachment Type, alarms if find_alarms has been called)
          type AttachmentData = dict[str, dict[str, dict[str, dict[str, dict[str, Any]]]]]
          CONTEXT_ACCOUNT_ID:str = ""    # Account ID this lambda is running under, determined by the initial context provided to the handler.
          logger = logging.getLogger()
          
          def timeout(event, context):
              """
              If we run over the lambda invocation time limit, this function is called. If we're called from CloudFormation, it sends the failed status back 
              to keep CloudFormation from waiting longer.
              """
              if 'ResponseURL' in event:
                  logger.error('Execution is about to time out, sending failure response to CloudFormation')
                  cfnresponse.send(event, context, cfnresponse.FAILED, {}, None)
              exit(1)
          
          
          def find_attachments() -> AttachmentData:
              """
              Find all CloudWAN attachments in the global network. This function can be relatively slow if a lot of regions are in use. It filters
              to only attachments that are available (fully working). Transit Gateway peering connections are also ignored (we will catch the traffic coming into
              transit gateway via whatever downstream connection).
          
              :return AttachmentData: Filled in AttachmentData, see description of type.
              """
              ret: AttachmentData = {"tgw": {}, "cwan": {}}
          
              # Find all Transit Gateway attachments. In NetworkManager mode, this requires cross-region calls, so batch together all TGWs in a region, then do one call per region.
              if os.environ['MODE'] == 'nm':
                  tgws_by_region: dict[str, list[str]] = {}
                  global_network_id = os.environ['GLOBAL_NETWORK_ARN'].split('/')[1]
                  nm = boto3.client('networkmanager')
                  for tgw in nm.get_transit_gateway_registrations(GlobalNetworkId=global_network_id)['TransitGatewayRegistrations']:
                      if 'TransitGatewayArn' in tgw:
                          tgw_region = tgw['TransitGatewayArn'].split(':')[3]
                          if tgw_region not in tgws_by_region:
                              tgws_by_region[tgw_region] = []
                          tgws_by_region[tgw_region].append(tgw['TransitGatewayArn'].split(':')[5].split('/')[1])
              else:
                  tgws_by_region = {os.environ['AWS_REGION']: [os.environ['TGW_ID']]}
          
              logger.debug(f'find_attachments() - discovered TGWs {tgws_by_region}')
          
              for region, tgw_list in tgws_by_region.items():
                  ec2 = boto3.client('ec2', region_name=region)
                  paginator = ec2.get_paginator('describe_transit_gateway_attachments')
                  resp = paginator.paginate(Filters=[{'Name': 'transit-gateway-id', 'Values': tgw_list}])
                  for page in resp:
                      logger.debug(f'find_attachments() - region {region}, TGWs {tgw_list}, processing response page {page}')
                      for attachment in page['TransitGatewayAttachments']:
                          if all(k in attachment for k in ['State', 'TransitGatewayId', 'TransitGatewayAttachmentId', 'ResourceType']):
                              if attachment['State'] == 'available' and attachment['ResourceType'] != 'peering':
                                  if region not in ret['tgw']:
                                      ret['tgw'][region] = {}
                                  if attachment['TransitGatewayId'] not in ret['tgw'][region]:
                                      ret['tgw'][region][attachment['TransitGatewayId']] = {}
                                  ret['tgw'][region][attachment['TransitGatewayId']][attachment['TransitGatewayAttachmentId']] = {'type': attachment['ResourceType']}
          
              # If using NetworkManager, find CloudWAN attachments. Find the core networks that are in this global network, then all attachments.
              if os.environ['MODE'] == 'nm':
                  global_network_id = os.environ['GLOBAL_NETWORK_ARN'].split('/')[1]
                  nm = boto3.client('networkmanager')
                  for core_network in nm.list_core_networks()['CoreNetworks']:
                      if 'GlobalNetworkId' in core_network and 'CoreNetworkId' in core_network:
                          if core_network['GlobalNetworkId'] == global_network_id:
                              paginator = nm.get_paginator('list_attachments')
                              resp = paginator.paginate(CoreNetworkId=core_network['CoreNetworkId'])
                              for page in resp:
                                  for attachment in page['Attachments']:
                                      if all(k in attachment for k in ['State', 'EdgeLocation', 'AttachmentType']):
                                          if attachment['State'] == 'AVAILABLE':
                                              if attachment['EdgeLocation'] not in ret['cwan']:
                                                  ret['cwan'][attachment['EdgeLocation']] = {}
                                              if core_network['CoreNetworkId'] not in ret['cwan'][attachment['EdgeLocation']]:
                                                  ret['cwan'][attachment['EdgeLocation']][core_network['CoreNetworkId']] = {}
                                              ret['cwan'][attachment['EdgeLocation']][core_network['CoreNetworkId']][attachment['AttachmentId']] = {'type': attachment['AttachmentType']}
          
              logger.debug(f'find_attachments() returning {ret}')
              return ret
          
          
          def find_alarms(ad:AttachmentData) -> AttachmentData:
              """
              Augment the given AttachmentData with a third field in AttachmentInfo named 'alarms', type dict, keys the individual fields then a dict with key 'present':boolean 
              if the alarm was found, 'name':str for the alarm name, 'region':str for the CloudWatch region the alarm is in, and key 'settings':dict for the current settings for that alarm.
              Any alarms found that are invalid (due to being incomplete for some reason or pointing to an attachment that no longer exists) are removed from CloudWatch.
          
              :param AttachmentData ad: AttachmentData from find_attachments
              :param List[str] fields: List of Cloudwatch metric fields to look for (i.e. BytesIn, BytesOut)
              
              :return AttachmentData: Input AD, augmented as described.
              """
              # Set up alarms structure, and remember where CloudWAN attachments are at region-wise since they always are in cloudwatch_cloudwatch_region for reporting.
              metric_names = os.environ['METRIC_NAMES'].split(',')
              cwan_regions:dict[str,str] = {}
              for type, type_data in ad.items():
                  for region, region_data in type_data.items():
                      for _, attachments in region_data.items():
                          for attachment_id, attachment_data in attachments.items():
                              attachment_data['alarms'] = {x: {'present': False, 'name': None, 'region': None, 'settings': {}} for x in metric_names}
                              if type == 'cwan':
                                  cwan_regions[attachment_id] = region
          
              region_list = set(ad['cwan'].keys()).union(set(ad['tgw'].keys()))
              if os.environ['MODE'] == 'nm':
                  # Ensure we have the CloudWAN metric region as well.
                  region_list.add(get_cloudwan_cloudwatch_region())
              
              for region in region_list:
                  cw = boto3.client('cloudwatch', region_name=region)
                  paginator = cw.get_paginator('describe_alarms')    
                  lambda_arn = get_alarm_lambda_arn(region, create_missing=False)
                  if lambda_arn is None:    
                      print(f"Looking for alarms in region {region}, but the lambda is not deployed there, so assuming no alarms are there.")
                  else:
                      # We assume if the alarm is using our Lambda as an action, it's one of our alarms.
                      resp = paginator.paginate(AlarmTypes=['MetricAlarm'], ActionPrefix=lambda_arn)
                      for page in resp:
                          for alarm in page['MetricAlarms']:
                              net_id = None
                              attachment_id = None
                              metric_name = None
                              type = None
                              set_region = region
                              settings = {'MINUTES': int(alarm['DatapointsToAlarm'])}
                              if 'Metrics' in alarm:
                                  for metric in alarm['Metrics']:
                                      # We have three metrics of interest - the MetricStat with the metric we're watchng, then two math expressions with 
                                      # the ANOMALY_DETECTION_BAND and the IF condition, we want to get the current values out of.
                                      # ANOMALY_DETECTION_BAND(m1, 5) or IF(m1>10000000 AND m1>MAX(e1),1,0)
                                      if 'Expression' in metric:
                                          ab = re.search(r'ANOMALY_DETECTION_BAND\(.*,([ 0-9]+)\)', metric['Expression'])
                                          if ab is not None:
                                              settings['STDDEV'] = int(ab.group(1))
                                          else:
                                              ie = re.search(r'IF\(.*>([0-9]+) AND .*\)', metric['Expression'])
                                              if ie is not None:
                                                  settings['MINIMUM_VALUE'] = int(ie.group(1))
                                      if 'MetricStat' in metric and 'Dimensions' in metric['MetricStat']['Metric'] and 'MetricName' in metric['MetricStat']['Metric']:
                                          metric_name = metric['MetricStat']['Metric']['MetricName']
                                          for dim in metric['MetricStat']['Metric']['Dimensions']:
                                              if dim['Name'] == 'TransitGateway':
                                                  type = 'tgw'
                                                  net_id = dim['Value']
                                              elif dim['Name'] == 'TransitGatewayAttachment':
                                                  attachment_id = dim['Value']
                                              elif dim['Name'] == 'CoreNetwork':
                                                  type = 'cwan'
                                                  net_id = dim['Value']
                                              elif dim['Name'] == 'Attachment':
                                                  attachment_id = dim['Value']
                                                  set_region = cwan_regions[attachment_id]
                              if type is None or net_id is None or attachment_id is None or metric_name is None or metric_name not in metric_names:
                                  logger.info(f"Found an alarm {alarm['AlarmName']} from this script, but it does not have the correct metrics set on it (type={type}, net_id={net_id}, attach_id={attachment_id}, metric_name={metric_name}). Removing it.")
                                  remove_alarm(region, alarm['AlarmName'])
                              elif type not in ad or set_region not in ad[type] or net_id not in ad[type][set_region] or attachment_id not in ad[type][set_region][net_id]:
                                  logger.info(f"Found an alarm {alarm['AlarmName']} from this script, but the attachment it points to is gone (type={type}, net_id={net_id}, attach_id={attachment_id}, metric_name={metric_name}). Removing it.")
                                  remove_alarm(region, alarm['AlarmName'])
                              else:     
                                  ad[type][set_region][net_id][attachment_id]['alarms'][metric_name] = {'present': True, 'region': region, 'settings': settings, 'name': alarm['AlarmName']}
          
              logger.debug(f'find_alarms() returning {ad}')
              return ad
          
          
          def set_alarm(type:str, region:str, net_id:str, attachment_id:str, metric_name:str) -> None:
              """
              Create our standard alarm in Cloudwatch.
          
              :param str type: cwan or tgw
              :param str region: Region name of the resource
              :param str net_id: net_id
              :param str attachment_id: attachment_id
              :param str metric_name: Name of the metric to set the alarm on
              """
          
              # If this is a CloudWAN attachment, we use cloudwan_cloudwatch_region, else we use region. Set the correct dimensions for our metric here as well.
              if type == 'tgw':
                  cw = boto3.client('cloudwatch', region_name=region)
                  lambda_arn = get_alarm_lambda_arn(region)
                  metric_stat = {'Metric': {'Namespace': 'AWS/TransitGateway', 'MetricName': metric_name, 
                                            'Dimensions': [{'Name': 'TransitGateway', 'Value': net_id}, {'Name': 'TransitGatewayAttachment', 'Value': attachment_id}]},
                                            'Period': 60, 'Stat': 'Average'}
              else:
                  cloudwan_cloudwatch_region = get_cloudwan_cloudwatch_region()
                  cw = boto3.client('cloudwatch', region_name=cloudwan_cloudwatch_region)
                  lambda_arn = get_alarm_lambda_arn(cloudwan_cloudwatch_region)
                  metric_stat = {'Metric': {'Namespace': 'AWS/Network Manager', 'MetricName': metric_name,
                                            'Dimensions': [{'Name': 'CoreNetwork', 'Value': net_id}, {'Name': 'Attachment', 'Value': attachment_id}]},
                                            'Period': 60, 'Stat': 'Average'}
          
              # This format is made such that humans can understand the alarm easily in console view, although it makes
              # the scripting a bit harder by not having a consistent prefix.
              alarm_name = f'{metric_name}-{type}-{attachment_id}-{os.environ['OBJECTS_NAME']}-Alarm'
          
              # Create the actual alarm. The metric is in m1, the anomaly band processing is in e1. e2 creates logic that ANDs together the anomaly band being exceeded,
              # and m1 being greater than our minimum_value, and outputs a 1 or 0, and is the value Cloudwatch is looking at. Thus, we set the alarming operator to 
              # greater than 0.
              cw.put_metric_alarm(AlarmName=alarm_name,
                                  AlarmDescription=f'Automatically generated alarm by {os.environ['STACK_ARN']}',
                                  OKActions=[lambda_arn], AlarmActions=[lambda_arn],
                                  Metrics=[{'Id': 'm1', 'MetricStat': metric_stat, 'ReturnData': False},
                                          {'Id': 'e1', 'Expression': f'ANOMALY_DETECTION_BAND(m1, {os.environ['STDDEV']})', 'Label': 'Anomaly Band', 'ReturnData': False},
                                          {'Id': 'e2', 'Expression': f'IF(m1>{os.environ['MINIMUM_VALUE']} AND m1>MAX(e1),1,0)', 'Label': 'Alarm Trigger', 'ReturnData': True}],
                                  Threshold=0, ComparisonOperator='GreaterThanThreshold', EvaluationPeriods=int(os.environ['MINUTES']), DatapointsToAlarm=int(os.environ['MINUTES']))
          
          
          def delete_alarm(type:str, region:str, attachment_id:str, metric_name:str) -> None:
              """
              Delete a standard alarm in Cloudwatch.
          
              :param str type: cwan or tgw
              :param str region: Region name of the resource
              :param str attachment_id: attachment_id
              :param str metric_name: Name of the metric to delete the alarm from
              """
              if type == 'tgw':
                  cw = boto3.client('cloudwatch', region_name=region)
              else:
                  cloudwan_cloudwatch_region = get_cloudwan_cloudwatch_region()
                  cw = boto3.client('cloudwatch', region_name=cloudwan_cloudwatch_region)    
          
              alarm_name = f'{metric_name}-{type}-{attachment_id}-{os.environ['OBJECTS_NAME']}-Alarm'
              # Make sure to set the alarm state to OK (which will trigger the OKAction if necessary) before deleting the alarm
              cw.set_alarm_state(AlarmName=alarm_name, StateValue='OK', StateReason="Alarm is being removed.")
              cw.delete_alarms(AlarmNames=[alarm_name])
          
          
          def get_partition_for_region(region_name:str) -> str:
              """
              A quick function to get the partition name for any given region.
          
              :param str region_name: Region name to get.
              :return str: The partition name ('aws', etc.)
              """
              session = boto3.Session()
              return session.get_partition_for_region(region_name)
          
          
          def get_cloudwan_cloudwatch_region() -> str:
              """
              As per https://docs.aws.amazon.com/network-manager/latest/cloudwan/cloudwan-cloudwatch-metrics.html, CloudWAN metrics are only
              available in one region per partition. This returns those values. 
          
              :param str region_name: Region name to find
              :return str: Region name CloudWAN's CloudWatch metrics are in
              """
              match os.environ['GLOBAL_NETWORK_ARN'].split(':')[1]:
                  case 'aws':
                      return 'us-west-2'
                  case 'aws-us-gov':
                      return 'us-gov-west-1'
                  case _:
                      # Could only get here if a partition has been added and this code isn't updated - raise an exception
                      raise Exception(f'Unhandled partition {os.environ['GLOBAL_NETWORK_ARN'].split(':')[1]}. The get_cloudwan_cloudwatch_region function needs updating against the AWS documentation.')
          
          
          def get_alarm_names(region_name:str) -> dict[str, str]:
              """
              Calculate the lambda and S3 names for the alarm function to use for a given region. Returns some additional bits that are
              needed anyway for the calculation
          
              :param str region_name: Region name to get names for
              :return dict[str, str]: Keys of 
                                      'bucket_name' and 'bucket_arn' for the S3 bucket, 
                                      'lambda_name' for the lambda name,
                                      'partition' for the partition of the region
              """
              ret:dict[str,str] = {}
              ret['partition'] = get_partition_for_region(region_name)
              ret['bucket_name'] = f'{CONTEXT_ACCOUNT_ID}-{os.environ['OBJECTS_NAME']}-{region_name}'
              ret['bucket_arn'] = f'arn:{ret['partition']}:s3:::{ret['bucket_name']}'
              ret['lambda_name'] = f'{os.environ['OBJECTS_NAME']}_Alarm_Handler'
              return ret
          
          
          def get_alarm_lambda_arn(region_name:str, create_missing:bool = True) -> Optional[str]:
              """
              Return the lambda ARN of our alarm lambda for the given region. Creates the lambda, if it's not already present.
          
              :param str region_name: Region name for the lambda
              :param bool create_missing: Create the lambda, if it's not present in the region yet.
              :return str: A full ARN (that may have just been created)
              """
          
              names = get_alarm_names(region_name)
              l = boto3.client('lambda', region_name=region_name)
          
              # See if the function already exists.  If it does, just return its ARN.
              try:
                  resp = l.get_function(FunctionName=names['lambda_name'])
                  return resp['Configuration']['FunctionArn']
              except botocore.exceptions.ClientError as e:
                  # This is likely ResourceNotFoundException because the function isn't here yet, but let's be sure.
                  if e.response['Error']['Code'] != 'ResourceNotFoundException':
                      raise e
              
              # Build out the new region - this function returns the ARN we need.
              if create_missing:
                  return region_build(dest_region_name=region_name)
              else:
                  return None
          
          
          def get_alarm_handler_lambda_code() -> bytes:
              """
              Return the data needed for a ZipFile input of a create_function lambda call, based on a copy of the AlarmLambda function in the local region.
              """
              names = get_alarm_names(region_name=os.environ['AWS_REGION'])
              l = boto3.client('lambda')
              lambda_data = l.get_function(FunctionName=names['lambda_name'])
              req = urllib.request.urlopen(urllib.request.Request(url=lambda_data['Code']['Location'], method='GET'), timeout=5)
              return req.read()
          
          
          def region_build(dest_region_name:str) -> str:
              """
              Set up a new region for use. This involves creating an S3 bucket and copying the policy from source region, then copying the lambda
              function from the source region.
          
              :param str dest_region_name: The region to copy to.
          
              :return str: The ARN of the lambda function in the new region.
              """
              source_names = get_alarm_names(os.environ['AWS_REGION'])
              dest_names = get_alarm_names(dest_region_name)
          
              # The lambda requires a regional S3 bucket - create one. Wrapped in a try block because there's a chance this bucket already exists
              # (this code is running multiple times for some reason).
              dest_s3 = boto3.client('s3', region_name=dest_region_name)
              try:
                  # Don't ask - I don't know why either.
                  if dest_region_name != 'us-east-1':
                      resp = dest_s3.create_bucket(Bucket=dest_names['bucket_name'], CreateBucketConfiguration={'LocationConstraint': dest_region_name})
                  else:
                      resp = dest_s3.create_bucket(Bucket=dest_names['bucket_name'])
                  logger.info(f'Expanding to a new region: Created a new bucket {dest_names['bucket_name']} in region {dest_region_name} for logs.')
          
              except botocore.exceptions.ClientError as e:
                  # If we already have and own the bucket, ignore and move on, otherwise raise exception.
                  if e.response['Error']['Code'] != 'BucketAlreadyOwnedByYou':
                      raise e        
          
              # Copy the bucket policy from source to dest, replacing region names.
              src_s3 = boto3.client('s3')
              src_policy = src_s3.get_bucket_policy(Bucket=source_names['bucket_name'])
              
              # Replace the region names in the policy. Replace region name in arn (this catches things like 'arn:aws:logs::us-west-2', etc)
              dst_policy = re.sub(fr'(arn:[a-z-]+:[a-z0-9-]*:){os.environ['AWS_REGION']}:', fr'\1:{dest_region_name}:', src_policy['Policy'])
              # Replace bucket ARNs:
              dst_policy = dst_policy.replace(source_names['bucket_arn'], dest_names['bucket_arn'])
              # Apply to destination.
              dest_s3.put_bucket_policy(Bucket=dest_names['bucket_name'], Policy=dst_policy)
              logger.info(f'Copied bucket policy from {source_names['bucket_name']} to {dest_names['bucket_name']}')
          
              # Copy bucket lifecycle configuration
              try:
                  resp = src_s3.get_bucket_lifecycle_configuration(Bucket=source_names['bucket_name'])    
                  dest_s3.put_bucket_lifecycle_configuration(Bucket=dest_names['bucket_name'], LifecycleConfiguration={'Rules': resp['Rules']})
                  logger.info(f'Copied bucket lifecycle configuration from {source_names['bucket_name']} to {dest_names['bucket_name']}')
              except botocore.exceptions.ClientError as e:
                  # Someone maybe deleted the lifecycle configuration we made on the original bucket? If so, just ignore it and move on.
                  if e.response['Error']['Code'] != 'NoSuchLifecycleConfiguration':
                      raise e
          
              # Create log group in destination
              lgn = f'/aws/lambda/{dest_names['objects_name']}-shared-logs'
              dest_logs = boto3.client('logs', region_name=dest_region_name)
              dest_logs.create_log_group(logGroupName=lgn)
              # Copy log group configuration
          
              try:
                  src_logs = boto3.client('logs')
                  lg = src_logs.get_log_group(lgn)
                  dest_logs.put_retention_policy(logGroupName=lgn, retentionInDays=lg.get('retentionInDays'))
              except Exception as e:
                  logger.info(f'Had an exception {e} trying to get or set the log group retention policy. Ignoring the error and continuing.')
          
              # Create the new lambda.
              l = boto3.client('lambda', region_name=dest_region_name)
              try:
                  resp = l.create_function(FunctionName=dest_names['lambda_name'], Runtime='python3.12', Role=os.environ['ALARM_LAMBDA_ROLE'], Handler='handler',
                                           Code={'ZipFile': get_alarm_handler_lambda_code()}, Timeout=5, DeadLetterConfig={'TargetArn': os.environ['SNS_TOPIC']},
                                           LoggingConfig={'LogGroup': lgn},
                                           Environment={'Variables': {'SNS_TOPIC': os.environ['SNS_TOPIC'], 'S3_BUCKET': dest_names['bucket_arn']}})
                  logger.info(f'Expanding to a new region: Created a new lambda {dest_names['lambda_name']} in region {dest_region_name}')
                  return resp['FunctionArn']
              except botocore.exceptions.ClientError as e:
                  # It's remotely possible a different lambda run beat us to this part - if so just get the (new) ARN.
                  if e.response['Error']['Code'] != 'ResourceConflictException':    
                      raise e
                  else:
                      # Retry the fetch. 
                      resp = l.get_function(FunctionName=dest_names['lambda_name'])
                      return resp['Configuration']['FunctionArn']
          
          
          def region_destroy(region_name: str) -> None:
              """
              Destroy everything in a region that was built by region_build.
          
              :param region_name:
              :return: None
              """
              # Lambda first.
              names = get_alarm_names(region_name)
              l = boto3.client('lambda', region_name=region_name)
              try:
                  l.delete_function(FunctionName=names['lambda_name'])
                  logger.info(f"Purge: Region {region_name}: Removed lambda {names['lambda_name']}")
              except botocore.exceptions.ClientError as e:
                  if e.response['Error']['Code'] != 'ResourceNotFoundException':
                      raise e
          
              # S3 bucket and its contents next.
              s3 = boto3.resource('s3')
              try:
                  bucket = s3.Bucket(names['bucket_name'])
                  bucket.objects.all().delete()
                  bucket.delete()
                  logger.info(f"Purge: Region {region_name}: Removed S3 bucket {names['bucket_name']}")
              except botocore.exceptions.ClientError as e:
                  if e.response['Error']['Code'] != 'NoSuchBucket':
                      raise e
          
              # We deliberately leave the log group.
          
          
          def remove_alarm(region_name: str, alarm_name: str) -> None:
              """
              Deletes an alarm we created.
              :param str region_name: Region of the CloudWatch instance with the alarm
              :param str alarm_name: Alarm name
              """
              cw = boto3.client('cloudwatch', region_name=region_name)
              # Make sure to set the alarm state to OK (which will trigger the OKAction if necessary) before deleting the alarm
              cw.set_alarm_state(AlarmName=alarm_name, StateValue='OK', StateReason="Alarm is being removed.")
              cw.delete_alarms(AlarmNames=[alarm_name])
          
          
          def do_resync():
              """
              Resync the state between alarms created in CloudWatch, and the attachments actually present. Handles new attachments and attachments that have been deleted.
              Also handles if environment parameters for the alarms (THRESHOLD, MINIMUM_VALUE, MINUTES) have been changed and updates the alarms to match.
              """
              # Gather our current state. This also removes any bad alarms that are still present for attachments that are not.
              ad = find_alarms(find_attachments())
          
              # What the alarm settings should be
              settings = {'MINIMUM_VALUE': int(os.environ['MINIMUM_VALUE']), 'STDDEV': int(os.environ['STDDEV']), 'MINUTES': int(os.environ['MINUTES'])}
          
              # Set all alarms that need to be set.
              for type, type_data in ad.items():
                  for region, region_data in type_data.items():
                      for net_id, attachments in region_data.items():
                          for attachment_id, attach_data in attachments.items():
                              for metric_name in os.environ['METRIC_NAMES'].split(','):
                                  if attach_data['alarms'][metric_name]['present'] is False:
                                      logger.info(f"Missing alarm for {region} attachment {attachment_id} metric {metric_name} - creating it.")
                                      set_alarm(type, region, net_id, attachment_id, metric_name)
                                  elif attach_data['alarms'][metric_name]['settings'] != settings:
                                      logger.info(f"Settings have changed for alarm {attach_data['alarms'][metric_name]['name']} - current settings are {attach_data['alarms'][metric_name]['settings']}, new settings are {settings}. Rebuilding the alarm.")
                                      remove_alarm(attach_data['alarms'][metric_name]['region'], attach_data['alarms'][metric_name]['name'])
                                      set_alarm(type, region, net_id, attachment_id, metric_name)
                                  else:
                                      logger.debug(f"Region {region}, type {type}, attachment {attachment_id}: Alarm {attach_data['alarms'][metric_name]['name']} is correct.")
          
          
          def do_purge():
              """
              Purge everything we've created as part of this.
              """
              # Delete all alarms.
              regions:set[str] = set()
              metric_names = os.environ['METRIC_NAMES'].split(',')
              ad = find_alarms(find_attachments())
              for type, type_data in ad.items():
                  for region, region_data in type_data.items():
                      regions.add(region)
                      for net_id, attachments in region_data.items():
                          for attachment_id, _ in attachments.items():
                              for metric_name in metric_names:
                                  if ad[type][region][net_id][attachment_id]['alarms'][metric_name] is True:
                                      alarm_name = f'{metric_name}-{type}-{attachment_id}-{os.environ['OBJECTS_NAME']}-Alarm'
                                      remove_alarm(region, alarm_name)
          
              # Delete the lambdas and buckets from all regions except ours (CloudFormation will handle that). 
              # We wrap this in try blocks as in case of aborted deployments or running at the same time, we could have already deleted these.
              if os.environ['MODE'] == 'nm':
                  regions.discard(os.environ['AWS_REGION'])
                  for region_name in regions:
                      region_destroy(region_name)
          
          
          
          def process_nm(event, context):
              """
              Process a notification from Network Manager. This event can be a few different types, and can be in any region, so this function needs to handle all
              of that.
              """
              # These events get raised similarly for TGW and CloudWAN, but with different elements in the detail section. See
              # https://docs.aws.amazon.com/network-manager/latest/tgwnm/monitoring-events.html for help decoding this section.
              match event['detail']['changeType']:
                  case 'VPN-CONNECTION-CREATED' | 'VPC-ATTACHMENT-CREATED' | 'DXGW-ATTACHMENT-CREATED' | 'CONNECT_ATTACHMENT_CREATED':
                      # CloudWAN will have edgeLocation, Transit Gateway has region.
                      if 'edgeLocation' in event['detail']:
                          for metric in os.environ['METRIC_NAMES']:
                              set_alarm(type='cwan', region=event['detail']['edgeLocation'], metric_name=metric,
                                        net_id=[s for s in event['resources'] if 'core-network' in s][0].split('/')[1],
                                        attachment_id=event['detail']['attachmentArn'].split('/')[1])
                      else:
                          for metric in os.environ['METRIC_NAMES']:
                              set_alarm(type='tgw', region=event['detail']['region'], metric_name=metric,
                                        net_id=event['detail']['transit-gateway-arn'].split('/')[1],
                                        attachment_id=event['detail']['transit-gateway-attachment-arn'].split('/')[1])            
                  case 'VPN-CONNECTION-DELETED' | 'VPC-ATTACHMENT-DELETED' | 'DXGW-ATTACHMENT-DELETED' | 'CONNECT_ATTACHMENT_DELETED':
                      # CloudWAN will have edgeLocation, Transit Gateway has region.
                      if 'edgeLocation' in event['detail']:
                          for metric in os.environ['METRIC_NAMES']:
                              delete_alarm(type='cwan', region=event['detail']['edgeLocation'], metric_name=metric,
                                           attachment_id=event['detail']['attachmentArn'].split('/')[1])
                      else:
                          for metric in os.environ['METRIC_NAMES']:
                              delete_alarm(type='tgw', region=event['detail']['region'], metric_name=metric,
                                           attachment_id=event['detail']['transit-gateway-attachment-arn'].split('/')[1])
                  case 'TGW-DELETED' | 'TGW-CREATED':
                      # Rethink the world.
                      do_resync()
                  case _:
                      # Ignore the other types (things like peering connections, etc)
                      pass
          
          
          def process_timer(event, context):
              # The timer fires off every so often (configurable) just to make sure some events didn't get missed. 
              # Do a resync to accomplish this.
              do_resync()
          
          
          def process_cloudformation(event, context):
              # If a CREATE, we need to do a resync which will catch every attachment that may have been missed.
              # If an UPDATE, we do a resync as well, which will correct any changes to settings of the alarms.
              # If a DELETE, delete all alarms that were created by this.
              if event['RequestType'] == 'Create':
                  do_resync()
              elif event['RequestType'] == 'Delete':
                  do_purge()
              elif event['RequestType'] == 'Update':
                  do_resync()
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Data": "Successful"})
          
          
          def handler(event: Dict[str, Any], context):
              timer = threading.Timer((context.get_remaining_time_in_millis() / 1000.00) - 0.5, timeout, args=[event, context])
              timer.start()
          
              global CONTEXT_ACCOUNT_ID
          
              try:
                  CONTEXT_ACCOUNT_ID = context.invoked_function_arn.split(':')[4]
          
                  if 'DEBUG' in event:
                      logger.setLevel(logging.DEBUG)
                      logger.debug(f"Debug mode active. Event: {event}. Environment: {os.environ}")
                  # Are we being called with a EventManager notification from Network Manager (this will only happen on CWAN type deployments)
                  if 'source' in event and event['source'] == 'aws.networkmanager':
                      logger.info(f"Processing Network Manager event, event data: {event}")
                      process_nm(event, context)
                  # Are we being called by EventManager from our timer?
                  elif 'source' in event and event['source'] == 'aws.events':
                      logger.info(f"Processing Event Bridge timer event, event data: {event}")
                      process_timer(event, context)
                  # Are we being called from CloudFormation as part of initial install, update, or delete?
                  elif 'StackId' in event:
                      logger.info(f"Processing CloudFormation event, event data: {event}")
                      process_cloudformation(event, context)
                  elif 'TEST' in event:
                      logger.info(f"Processing manual test request: {event}")
                      do_resync()
                  else:
                      # Likely testing but didn't give correct parameters.
                      logger.error(f"Lambda function called but event does not contain NetworkManager, EventBridge, or CloudFormation data. If you want to do a manual resync, pass a variable 'TEST' with any value. Aborting.")
                  return
          
              except Exception as e:
                  logger.error('Exception: %s' % e, exc_info=True)
                  logger.error(f"Data from request:\nevent={event}\ncontext={context}")
          
                  if 'ResponseURL' in event:
                      cfnresponse.send(event, context, cfnresponse.FAILED, {"e": f'{e}'}, reason=f'{e}')
          
                  # Do a best effort notification of the exception
                  sns = boto3.client('sns')
                  sns.publish(TopicArn=os.environ['SNS_TOPIC'], Subject=f'Lambda {os.environ['AWS_LAMBDA_FUNCTION_NAME']} in {os.environ['AWS_REGION']} had an exception.',
                      Message=f'The lambda function {os.environ['AWS_LAMBDA_FUNCTION_NAME']} in {os.environ['AWS_REGION']} had an exception.\n\n'
                      f'Exception: {e}\n\n'
                      f'Event data: {event}')
                  
                  raise e
        

  EventHandlerCall:
    Type: Custom::CleanupFunction
    Properties:
      ServiceToken: !GetAtt EventHandlerLambda.Arn
      # Trigger updates whenever these parameters change:
      MinValue: !Ref MinimumValue
      Minutes: !Ref MinutesBeforeStarting
      StdDev: !Ref StdDev

  EventHandlerResyncRule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub ${AWS::StackName}-ResyncEvents
      ScheduleExpression: 'rate(60 minutes)'
      Targets:
        - Id: 1
          Arn: !GetAtt EventHandlerLambda.Arn

  EventHandlerEventBridgePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref EventHandlerLambda
      Action: lambda:InvokeFunction
      Principal: "events.amazonaws.com"
      SourceAccount: !Sub ${AWS::AccountId}

  AlarmPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref AlarmLambda
      Action: lambda:InvokeFunction
      Principal: "lambda.alarms.cloudwatch.amazonaws.com"
      SourceAccount: !Sub ${AWS::AccountId}

  AlarmIAMPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      Description: !Sub Permissions needed for ${AWS::StackName}-AlarmLambda to function
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: GetCWData
            Effect: Allow
            Action:
              - "cloudwatch:GetMetricData"
            Resource: "*"
          - Sid: AllowFlowLogManipulation
            Effect: Allow
            Resource: "*"
            Action:
              - "ec2:DescribeFlowLogs"
              - "ec2:CreateFlowLogs"
              - "ec2:DeleteFlowLogs"
              - "ec2:CreateTags"
              - "logs:CreateLogDelivery"

  AlarmRole:
    Type: AWS::IAM::Role
    Properties:
      Description: !Sub Role for ${AWS::StackName}, region ${AWS::Region}, lambda AlarmLambda
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal: {Service: [lambda.amazonaws.com]}
          Action: ['sts:AssumeRole']
      Path: "/"
      ManagedPolicyArns:
        - !Sub "arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
        - !Ref LambdaCommonPolicy
        - !Ref AlarmIAMPolicy

  AlarmLambda:
    Type: AWS::Lambda::Function
    Properties:
      DeadLetterConfig:
        TargetArn: !If [ MakeSNSTopic, !Ref SNSTopic, !Ref ExistingSNSTopic ]
      LoggingConfig:
        LogGroup: !Ref SharedLambdaLogGroup
      Handler: index.handler
      Role: !GetAtt AlarmRole.Arn
      Runtime: python3.12
      Timeout: 30
      FunctionName: !Sub "${ObjectsName}_Alarm_Handler"
      Environment:
        Variables:
          SNS_TOPIC: !If [MakeSNSTopic,!Ref SNSTopic,!Ref ExistingSNSTopic]
          S3_BUCKET: !Ref S3Bucket
      Code:
        ZipFile: |
          """
          Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
          SPDX-License-Identifier: MIT-0
          
          alarm_handler.py
          
          Python lambda script to handle CloudWatch alarms being set and cleared, triggering captures on the affected
          Transit Gateway attachment.
          
          This script relies on environment variables to pass in how things work. Those variables are:
          
          S3_BUCKET - The full ARN of the bucket and path to save flow logs into
          SNS_TOPIC - The SNS topic ARN to send notifications to
          
          Additionally, it leverages some variables set by AWS Lambda itself - AWS_LAMBDA_FUNCTION_NAME and AWS_REGION.
          
          API references:
          https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2/client/create_flow_logs.html
          https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2/client/describe_flow_logs.html
          """
          import boto3
          import logging
          import os
          import datetime
          
          # Change this to be whatever fields you like. The default, provided here, are all fields.
          # See https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html#flow-log-records for information.
          TGW_FLOW_LOG_FORMAT = '${version} ${resource-type} ${account-id} ${tgw-id} ${tgw-attachment-id} ${tgw-src-vpc-account-id} ${tgw-dst-vpc-account-id} ${tgw-src-vpc-id} ${tgw-dst-vpc-id} ${tgw-src-subnet-id} ${tgw-dst-subnet-id} ${tgw-src-eni} ${tgw-dst-eni} ${tgw-src-az-id} ${tgw-dst-az-id} ${tgw-pair-attachment-id} ${srcaddr} ${dstaddr} ${srcport} ${dstport} ${protocol} ${packets} ${bytes} ${start} ${end} ${log-status} ${type} ${packets-lost-no-route} ${packets-lost-blackhole} ${packets-lost-mtu-exceeded} ${packets-lost-ttl-expired} ${tcp-flags} ${region} ${flow-direction} ${pkt-src-aws-service} ${pkt-dst-aws-service}'
          CREATOR_TAG = f"Lambda {os.environ['AWS_LAMBDA_FUNCTION_NAME']} in {os.environ['AWS_REGION']}"
          
          def get_flow_log_status(ec2: object, resource_id: str) -> bool:
              """
              Determine current flow log status
          
              :param ec2: Boto3 EC2 Client
              :param resource_id: As per create_flow_log ResourceId
          
              :return: Return True if a flow log is already active for this object, False otherwise.
              """
              fls = ec2.describe_flow_logs(Filters=[{'Name': 'resource-id', 'Values': [resource_id]}])
              for fl in fls['FlowLogs']:
                  if fl['ResourceId'] == resource_id:
                      if fl['FlowLogStatus'] == 'ACTIVE' and 'Tags' in fl:
                              for tag in fl['Tags']:
                                  if tag['Key'] == 'Creator' and tag['Value'] == CREATOR_TAG:
                                      return True
              return False
          
          
          def format_bytes(bytes_value: float) -> str:
              """
              Format bytes into human readable format.
              :param bytes_value: Number of bytes
              :return: Formatted string
              """
              for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
                  if bytes_value < 1024.0:
                      return f"{bytes_value:.1f} {unit}"
                  bytes_value /= 1024.0
              return f"{bytes_value:.1f} PB"
          
          
          def convert_to_metric(d: dict) -> dict:
              # The AlarmData has the metric info, but:
              # - Every key is lowercase instead of upper like we need to GetMetricStats
              # - It's Name instead of MetricName
              # - The Dimensions are in a different format.
              # Correct this.
              ret = {}
              for k, v in d.items():
                  newk = k[0].upper() + k[1:]
                  if newk == 'Name':
                      newk = 'MetricName'
                  if isinstance(v, dict):
                      if newk == 'Dimensions':
                          ret[newk] = [{'Name': k, 'Value': v} for k, v in convert_to_metric(v).items()]
                      else:
                          ret[newk] = convert_to_metric(v)
                  else:
                      if newk == 'ReturnData':
                          ret[newk] = True
                      else:
                          ret[newk] = v
          
              return ret
          
          
          def get_alarming_metric_str(alarm_data: dict) -> str:
              # We have a composite alarm - while the alarm we create is named something like BytesOut-tgw-tgw-attach-0ab9d8b7bcab228d4-alarms-Alarm,
              # that by itself doesn't help - it's two 1 or 0 evaluations. We have to get the alarm configuration and get the 'm1' metric to get the
              # actual values. Start with getting the metric.
              cw = boto3.client('cloudwatch')
              alarming_metric = None
          
              # Get metric dimensions
              for m in alarm_data['configuration']['metrics']:
                  if 'metricStat' in m:
                      alarming_metric = convert_to_metric(m)
                      break
              if alarming_metric is None:
                  print(f'Unable to find the metric for alarm_data {alarm_data}')
                  return ''
          
              # Get most recent data point and detection band for that metric
              endtime = datetime.datetime.now(datetime.timezone.utc)
              starttime = endtime - datetime.timedelta(minutes=15)
              mdq = [alarming_metric, {"Id": "ab1", "Expression": "ANOMALY_DETECTION_BAND(m1)"}]
              metric_data = cw.get_metric_data(MetricDataQueries=mdq, EndTime=endtime, StartTime=starttime, MaxDatapoints=1, ScanBy='TimestampDescending')
              if len(metric_data['MetricDataResults']) == 0 or len(metric_data['MetricDataResults'][0]['Values']) == 0:
                  print(f'Unable to get metric data for alarm_data {alarm_data}')
                  return ''
          
              # Get current value and the high water AD band. Convert units for a couple common metrics.
              cur_value = None
              high_band = None
              for md in metric_data['MetricDataResults']:
                  if md['Id'] == 'm1':
                      cur_value = md['Values'][0]
                  elif md['Id'] == 'ab1' and md['Label'][-4:] == 'High':
                      high_band = md['Values'][0]
          
              if alarming_metric['MetricStat']['Metric']['MetricName'] in ('BytesIn', 'BytesOut'):
                  cur_value = format_bytes(cur_value)
                  high_band = format_bytes(high_band)
          
              # We have everything now - return string
              return f"Alarm Information:\nAlarm: {alarm_data['alarmName']}\nCurrent value: {cur_value} per minute\nAnomaly detection high band: {high_band} per minute.\n\n"
          
          
          def disable_flow_log(ec2: object, sns: object, resource_type: str, resource_id: str, s3_dest_arn: str, logging_sns: str, alarm_data: dict):
              """
              Disable a flow log for the given resource.
              :param ec2: Boto3 EC2 Client
              :param sns: Boto3 SNS Client
              :param resource_type: As per create_flow_log ResourceType
              :param resource_id: As per create_flow_log ResourceId
              :param s3_dest_arn: S3 destination ARN, ala arn:aws:s3:::bucket/subfolder/
              :param logging_sns: SNS topic to send state changes to.
              """
              fls = ec2.describe_flow_logs(Filters=[{'Name': 'resource-id', 'Values': [resource_id]}])
              start_time = None
              for fl in fls['FlowLogs']:
                  if fl['ResourceId'] == resource_id and fl['FlowLogStatus'] == 'ACTIVE' and 'Tags' in fl:
                      for tag in fl['Tags']:
                          if tag['Key'] == 'Creator' and tag['Value'] == CREATOR_TAG:
                              ret = ec2.delete_flow_logs(FlowLogIds=[fl['FlowLogId']])
                              if len(ret['Unsuccessful']):
                                  raise Exception(f"Unable to delete flow log for {resource_type}, {resource_id}, at {s3_dest_arn}: {ret['Unsuccessful'][0]['Error']}")
                              start_time = fl['CreationTime']
              if start_time is None:
                  print(f"disable_flow_log called for a flow log that doesn't exist ({resource_type}, {resource_id}, {s3_dest_arn}). Ignoring.")
                  return
          
              print(f'Stopped flow log for {resource_type}, {resource_id}, at {s3_dest_arn}')
          
              # Build the S3 path to help the person receiving the email out. The path is based on date in UTC - get that.
              utc = datetime.datetime.now(datetime.timezone.utc)
              account_id = boto3.client('sts').get_caller_identity().get('Account')
              s3_path = f's3://{s3_dest_arn.split(':')[-1]}/AWSLogs/{account_id}/vpcflowlogs/{os.environ['AWS_REGION']}/{utc.year}/{utc.month:02}/{utc.day:02}'
          
              # How long did we have the flow log for?
              end_time = datetime.datetime.now(start_time.tzinfo)
              delta_time = end_time - start_time
          
              sns.publish(TopicArn=logging_sns, Subject=f'Disabled flow log for {resource_type}, {resource_id}',
                          Message=f'Flow logging has been disabled on {resource_type} ID {resource_id} to {s3_dest_arn}. It ran for {delta_time}.\n\n'
                                  f'You can view the logs collected by running aws s3 ls {s3_path}/\n\n'
                                  f'{get_alarming_metric_str(alarm_data)}'
                                  f'This message is from lambda function {os.environ['AWS_LAMBDA_FUNCTION_NAME']} in region {os.environ['AWS_REGION']}.')
          
          
          def enable_flow_log(ec2: object, sns: object, resource_type: str, resource_id: str, s3_dest_arn: str, logging_sns: str, alarm_data: dict):
              """
              Enable a flow log for the given resource.
              :param ec2: Boto3 EC2 Client
              :param sns: Boto3 SNS Client
              :param resource_type: As per create_flow_log ResourceType
              :param resource_id: As per create_flow_log ResourceId
              :param s3_dest_arn: S3 destination ARN, ala arn:aws:s3:::bucket/subfolder/
              :param logging_sns: SNS topic to send state changes to.
              """
              client_token = f'{resource_type}-{resource_id}-enable'
              ret = ec2.create_flow_logs(ClientToken=client_token, ResourceType=resource_type, ResourceIds=[resource_id],
                                         LogDestinationType='s3', LogDestination=s3_dest_arn, LogFormat=TGW_FLOW_LOG_FORMAT,
                                         TagSpecifications=[{'ResourceType': 'vpc-flow-log', 'Tags': [{'Key': 'Creator', 'Value': f'Lambda {os.environ['AWS_LAMBDA_FUNCTION_NAME']} in {os.environ['AWS_REGION']}'}]}])
          
              if len(ret['Unsuccessful']):
                  raise Exception(f"Unable to create flow log for {resource_type}, {resource_id}, at {s3_dest_arn}: {ret['Unsuccessful'][0]['Error']}")
          
              print(f'Started flow log for {resource_type}, {resource_id}, at {s3_dest_arn}')
          
              # Build the S3 path to help the person receiving the email out. The path is based on date in UTC - get that.
              utc = datetime.datetime.now(datetime.timezone.utc)
              account_id = boto3.client('sts').get_caller_identity().get('Account')
              s3_path = f's3://{s3_dest_arn.split(':')[-1]}/AWSLogs/{account_id}/vpcflowlogs/{os.environ['AWS_REGION']}/{utc.year}/{utc.month:02}/{utc.day:02}'
          
              sns.publish(TopicArn=logging_sns, Subject=f'Enabled flow log for {resource_type}, {resource_id}',
                          Message=f'Flow logging has been enabled on {resource_type} ID {resource_id} to {s3_dest_arn}.\n\n'
                                  f'You can view the logs collected by running: aws s3 ls {s3_path}/\n\n'
                                  f'{get_alarming_metric_str(alarm_data)}'
                                  f'This message is from lambda function {os.environ['AWS_LAMBDA_FUNCTION_NAME']} in region {os.environ['AWS_REGION']}.')
          
          def set_flow_log(ec2: object, sns: object, resource_type: str, resource_id: str, s3_dest_arn: str, enabled: bool, logging_sns: str, alarm_data: dict):
              """
              Ensures a flow log is set for the given resource. Adds or deletes as needed, but won't do anything if the
              state is already correct.
          
              :param ec2: Boto3 EC2 Client
              :param sns: Boto3 SNS Client
              :param resource_type: As per create_flow_log ResourceType
              :param resource_id: As per create_flow_log ResourceId
              :param s3_dest_arn: S3 destination ARN, ala arn:aws:s3:::bucket/subfolder/
              :param enabled: True to ensure the log is enabled, False to ensure it is disabled.
              :param logging_sns: SNS topic to send state changes to.
              :return:
              """
          
              # See if we have a log going currently.
              current_status = get_flow_log_status(ec2, resource_id)
          
              # Do we need to do a state change? If so, do it.
              if current_status and not enabled:
                  disable_flow_log(ec2, sns, resource_type, resource_id, s3_dest_arn, logging_sns, alarm_data)
              elif not current_status and enabled:
                  enable_flow_log(ec2, sns, resource_type, resource_id, s3_dest_arn, logging_sns, alarm_data)
              else:
                  print(f'No flow log state change needed for {resource_type}, {resource_id}, at {s3_dest_arn} - current status is {current_status}, enabled is {enabled}')
          
          
          def handler(event, context):
              try:
                  ec2 = boto3.client('ec2')
                  sns = boto3.client('sns')
          
                  # See https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html for an example o
                  # the event object that is being sent here.
                  print(f"Processing alarm event. Event data {event}")
          
                  # Make sure this is a Transit Gateway attachment.
                  if event['alarmData']['configuration']['metrics'][0]['metricStat']['metric']['namespace'] == 'AWS/TransitGateway':
                      tgw_attachment_id = event['alarmData']['configuration']['metrics'][0]['metricStat']['metric']['dimensions']['TransitGatewayAttachment']
                      # This will be True if we're going into alarm, or False when we're coming out of it.
                      set_or_reset = event['alarmData']['state']['value'] == 'ALARM'
                      # Where will this go?
                      s3_dest_arn = f'arn:aws:s3:::{os.environ['S3_BUCKET']}/transit_gateway_logs/{tgw_attachment_id}'
                      set_flow_log(ec2, sns, 'TransitGatewayAttachment', tgw_attachment_id, s3_dest_arn, set_or_reset, os.environ['SNS_TOPIC'], event['alarmData'])
                  return
          
              except Exception as e:
                  logging.error('Exception: %s' % e, exc_info=True)
                  logging.error(f"Data from request:\nevent={event}\ncontext={context}")
          
                  # This next line could error again on us, but CFN has already been responded to, so this is a 'best-effort'
                  # type call.
                  sns.publish(TopicArn=os.environ['SNS_TOPIC'], Subject=f'Lambda {os.environ['AWS_LAMBDA_FUNCTION_NAME']} in {os.environ['AWS_REGION']} had an exception.',
                              Message=f'The lambda function {os.environ['AWS_LAMBDA_FUNCTION_NAME']} in {os.environ['AWS_REGION']} had an exception.\n\n'
                              f'Exception: {e}\n\n'
                              f'Event data: {event}')
                  raise e
